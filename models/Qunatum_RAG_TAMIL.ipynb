{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Install necessary libraries if not already present\n",
        "try:\n",
        "    import pennylane as qml\n",
        "except ImportError:\n",
        "    !pip install pennylane\n",
        "    import pennylane as qml\n",
        "\n",
        "\n",
        "class EfficientTamilRAG:\n",
        "\n",
        "   def __init__(self, data_path='/content/dataset_KissanVanni_tamil.csv', sample_size=20000):\n",
        "        \"\"\"\n",
        "        Initialize RAG system with a subset of the data\n",
        "        \"\"\"\n",
        "        # Load and sample the dataset\n",
        "        self.df = pd.read_csv(data_path)\n",
        "        if len(self.df) > sample_size:\n",
        "            self.df = self.df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
        "\n",
        "        # Initialize models\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.embedding_model = SentenceTransformer(\n",
        "            'paraphrase-multilingual-MiniLM-L12-v2',\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Precompute embeddings in batches to save memory\n",
        "        self._precompute_embeddings()\n",
        "\n",
        "        # Initialize smaller generation model for efficiency\n",
        "        self.generator = pipeline(\n",
        "            'text-generation',\n",
        "            model='google/flan-t5-base', # Example: Flan-T5\n",
        "            device=0 if self.device == 'cuda' else -1\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def _precompute_embeddings(self, batch_size=256):\n",
        "      \"\"\"Compute embeddings in batches to manage memory, potentially using quantum embeddings\"\"\"\n",
        "      print(\"Precomputing embeddings...\")\n",
        "      start_time = time.time()\n",
        "\n",
        "      self.question_embeddings = []\n",
        "      for i in range(0, len(self.df), batch_size):\n",
        "          batch = self.df['question'].iloc[i:i+batch_size].tolist()\n",
        "          # Classical Embeddings\n",
        "          batch_embeddings = self.embedding_model.encode(\n",
        "              batch,\n",
        "              convert_to_tensor=True,\n",
        "              show_progress_bar=False\n",
        "          )\n",
        "          self.question_embeddings.append(batch_embeddings.cpu().numpy()) #Keep classical as a backup\n",
        "\n",
        "      self.question_embeddings = np.concatenate(self.question_embeddings)\n",
        "      print(f\"Embeddings computed in {time.time()-start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "    def quantum_embedding_function(self, classical_embeddings):\n",
        "        \"\"\"\n",
        "        This is a placeholder for a quantum embedding function.\n",
        "        You would replace this with your actual quantum embedding algorithm.\n",
        "        \"\"\"\n",
        "\n",
        "        dev = qml.device(\"default.qubit\", wires=4)\n",
        "\n",
        "        @qml.qnode(dev)\n",
        "        def circuit(inputs):\n",
        "          # quantum circuit goes here\n",
        "\n",
        "          # example\n",
        "          qml.RX(inputs[0], wires=0)\n",
        "          qml.RY(inputs[1], wires=1)\n",
        "          return [qml.expval(qml.PauliZ(i)) for i in range(4)]\n",
        "\n",
        "        quantum_embeddings = []\n",
        "        for embedding in classical_embeddings:\n",
        "          quantum_embeddings.append(circuit(embedding[:2])) # replace with your quantum embedding logic\n",
        "\n",
        "        return np.array(quantum_embeddings)\n"
      ],
      "metadata": {
        "id": "vDJJOOyuw_Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: any query as input and respond from rag\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize with limited rows\n",
        "    rag = EfficientTamilRAG(sample_size=20000)\n",
        "\n",
        "    # Get user query as input\n",
        "    user_query = input(\"Enter your Tamil agricultural query: \")\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Query: {user_query}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    result = rag.generate(user_query)\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    print(f\"Answer ({elapsed:.2f}s): {result['answer']}\")\n",
        "    print(\"\\nRetrieved Context:\")\n",
        "    for i, ctx in enumerate(result['context'], 1):\n",
        "        print(f\"{i}. [Score: {ctx['score']:.3f}] {ctx['question']}\")\n",
        "        print(f\"   {ctx['answer']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "444wFeojujXU",
        "outputId": "328319e6-937a-47f2-f782-c6060ea1d997"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precomputing embeddings...\n",
            "Embeddings computed in 9.12 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Tamil agricultural query: அமெரிக்காவில் எந்தெந்த நாடுகளில் மீன் பண்ணைகள் உள்ளன\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Query: அமெரிக்காவில் எந்தெந்த நாடுகளில் மீன் பண்ணைகள் உள்ளன\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer (0.26s): \n",
            "\n",
            "Retrieved Context:\n",
            "1. [Score: 1.000] அமெரிக்காவில் எந்தெந்த நாடுகளில் மீன் பண்ணைகள் உள்ளன\n",
            "   கலிபோர்னியா, இடாஹோ, அலபாமா, ஆர்கன்சாஸ், லூசியானா, மிசிசிப்பி மற்றும் தென்கிழக்கு யு.எஸ் கடற்கரையில்\n",
            "2. [Score: 1.000] அமெரிக்காவில் எந்தெந்த நாடுகளில் மீன் பண்ணைகள் உள்ளன\n",
            "   கலிபோர்னியா, இடாஹோ, அலபாமா, ஆர்கன்சாஸ், லூசியானா, மிசிசிப்பி மற்றும் தென்கிழக்கு யு.எஸ் கடற்கரையில்\n",
            "3. [Score: 1.000] அமெரிக்காவில் எந்தெந்த நாடுகளில் மீன் பண்ணைகள் உள்ளன\n",
            "   கலிபோர்னியா, இடாஹோ, அலபாமா, ஆர்கன்சாஸ், லூசியானா, மிசிசிப்பி மற்றும் தென்கிழக்கு யு.எஸ் கடற்கரையில்\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save the raq system for future use\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Assuming 'rag' is your initialized EfficientTamilRAG object\n",
        "with open('rag_system.pkl', 'wb') as f:\n",
        "    pickle.dump(rag, f)\n"
      ],
      "metadata": {
        "id": "JObi1B9LvrmB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "\n",
        "# Load the saved model\n",
        "with open('rag_system.pkl', 'rb') as f:\n",
        "    rag = pickle.load(f)\n",
        "\n",
        "# Get user query as input\n",
        "user_query = input(\"Enter your Tamil agricultural query: \")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Query: {user_query}\")\n",
        "\n",
        "start_time = time.time()\n",
        "result = rag.generate(user_query)\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"Answer ({elapsed:.2f}s): {result['answer']}\")\n",
        "print(\"\\nRetrieved Context:\")\n",
        "for i, ctx in enumerate(result['context'], 1):\n",
        "    print(f\"{i}. [Score: {ctx['score']:.3f}] {ctx['question']}\")\n",
        "    print(f\"   {ctx['answer']}\")\n"
      ],
      "metadata": {
        "id": "jrZ55CL_v9J6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}